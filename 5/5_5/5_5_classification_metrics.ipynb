{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5. Metryki specyficzne dla klasyfikacji binarnej\n",
    "\n",
    "Stworzyliśmy w poprzednim rozdziale prosty model, który klasyfikował nam punkty na podstawie jednej tylko cechy. Byliśmy w stanie graficznie dostrzec jak model będzie klasyfikował nowe przykłady, ale w jaki sposób moglibyśmy sprawdzić model, gdyby cech było więcej? Musimy znać metody, które pozwolą nam zmierzyć jakość stworzonego systemu i dodatkowo będziemy mogli na ich podstawie wybrać najlepszy, jeśli akurat będziemy porównywać kilka różnych rozwiązań."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład klasyfikacji medycznej\n",
    "\n",
    "Załóżmy, że tworzymy system, który ma na celu diagnozować pacjentów na podstawie ich wyników badań. Nie będzie to system ogólnego użytku, ale diagnozujący jedną konkretną chorobę, np. pewien specyficzny typ nowotworu. Klasą pozytywną będzie tutaj obecność tego schorzenia. Wyjątkowo, nie będziemy skupiać się cechach i sposobie ich wykorzystania przez model, a jedynie stworzymy sobie pięć przykładów modeli, które będą popełniały innego rodzaju błędy i zobaczymy jak przełoży się to na wartość poszczególnych metryk.\n",
    "\n",
    "Na potrzeby tego eksperymentu zakładamy, że mamy zbiór testowy opisujący 1000 pacjentów, a choroba dotyka 1,7% populacji, tj. w naszej próbie powinno być 17 takich osób. Skorzystamy jednak z generatora liczb pseudolosowych, aby wygenerować sobie takie dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    982\n",
       "1     18\n",
       "Name: real_class, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(63)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"real_class\": np.random.choice([0, 1], size=1000, \n",
    "                                   p=[.983, .017])\n",
    "})\n",
    "predictions_df[\"real_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Musimy wprowadzić sobie kilka pojęć, które często będą przewijać się w pracy z metodami ML. Pod pojęciem *confusion matrix* (pol. tablica pomyłek) kryje się forma prezentacji predykcji dla modelu klasyfikacji binarnej.\n",
    "\n",
    "![](images/confusion-matrix.png)\n",
    "\n",
    "Będziemy stopniowo wprowadzać kolejne metryki, ale na początek omówmy modele, które chcielibyśmy porównać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacja scikit-learn wykorzystuje trochę inną konwencję zapisu, niż zaprezentowaliśmy powyżej:\n",
    "- $ C_{0,0} $ - $ TN $\n",
    "- $ C_{1,0} $ - $ FN $\n",
    "- $ C_{0,1} $ - $ FP $\n",
    "- $ C_{1,1} $ - $ TP $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Model zupełnie losowy\n",
    "\n",
    "W naszym problemie mamy dwie klasy i podstawowym podejściem może być stworzenie modelu, który losowo zwraca wartość $ 0 $ bądź $ 1 $, nie zwracając przy tym żadnej uwagi na rzeczywiste częstotliwości występowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_class</th>\n",
       "      <th>pred_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     real_class  pred_a\n",
       "370           0       0\n",
       "688           0       1\n",
       "403           0       0\n",
       "726           0       1\n",
       "921           0       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[\"pred_a\"] = np.random.choice([0, 1], \n",
    "                                            size=1000, \n",
    "                                            p=[.5, .5])\n",
    "predictions_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[473, 509],\n",
       "       [  8,  10]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predictions_df[\"real_class\"], \n",
    "                 predictions_df[\"pred_a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Model losowy ważony\n",
    "\n",
    "Znamy częstotliwość występowania choroby w populacji, więc możemy też stworzyć drugi model, który będzie również losowo generował odpowiedzi, ale uwzględniając tę wartość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_class</th>\n",
       "      <th>pred_a</th>\n",
       "      <th>pred_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     real_class  pred_a  pred_b\n",
       "493           0       1       0\n",
       "531           0       0       0\n",
       "171           0       1       0\n",
       "596           0       1       0\n",
       "210           0       1       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[\"pred_b\"] = np.random.choice([0, 1], \n",
    "                                            size=1000, \n",
    "                                            p=[.983, .017])\n",
    "predictions_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[966,  16],\n",
       "       [ 17,   1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predictions_df[\"real_class\"], \n",
    "                 predictions_df[\"pred_b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model negatywny\n",
    "\n",
    "Choroba jest na tyle rzadka, że stworzony model mógłby nauczyć się zawsze zwracać klasę negatywną ($ 0 $). Sprawdźmy również taki scenariusz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_class</th>\n",
       "      <th>pred_a</th>\n",
       "      <th>pred_b</th>\n",
       "      <th>pred_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     real_class  pred_a  pred_b  pred_c\n",
       "104           0       0       0       0\n",
       "357           0       0       0       0\n",
       "202           0       1       0       0\n",
       "82            0       0       0       0\n",
       "631           0       0       1       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[\"pred_c\"] = 0\n",
    "predictions_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[982,   0],\n",
       "       [ 18,   0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predictions_df[\"real_class\"], \n",
    "                 predictions_df[\"pred_c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Model pozytywny\n",
    "\n",
    "Odwróćmy poprzedni scenariusz i stwórzmy model, który zawsze będzie twierdził, że nasz pacjent jest chory. Dzięki temu lekarz będzie zmuszony dokonać kolejnych, bardziej szczegółowych badań i ewentualnie wykluczyć chorobę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_class</th>\n",
       "      <th>pred_a</th>\n",
       "      <th>pred_b</th>\n",
       "      <th>pred_c</th>\n",
       "      <th>pred_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     real_class  pred_a  pred_b  pred_c  pred_d\n",
       "109           0       0       0       0       1\n",
       "243           0       1       0       0       1\n",
       "154           0       1       0       0       1\n",
       "150           0       1       0       0       1\n",
       "300           0       0       0       0       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[\"pred_d\"] = 1\n",
    "predictions_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 982],\n",
       "       [  0,  18]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predictions_df[\"real_class\"], \n",
    "                 predictions_df[\"pred_d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Model kontrolny\n",
    "\n",
    "Stwórzymy też, poza wszystkimi poprzednimi przypadkami, jeden model kontrolny, który hipotetycznie mógłby zostać nauczony w projekcie ML. Nie będzie on działał idealnie, ale do jego wygenerowania wykorzystamy rzeczywiste klasy każdej z obserwacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_class</th>\n",
       "      <th>pred_a</th>\n",
       "      <th>pred_b</th>\n",
       "      <th>pred_c</th>\n",
       "      <th>pred_d</th>\n",
       "      <th>pred_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     real_class  pred_a  pred_b  pred_c  pred_d  pred_e\n",
       "657           0       0       0       0       1       0\n",
       "401           0       0       0       0       1       0\n",
       "402           0       0       0       0       1       0\n",
       "43            0       1       0       0       1       0\n",
       "773           0       1       0       0       1       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[\"pred_e\"] = predictions_df[\"real_class\"] \\\n",
    "    .where(np.random.random(size=1000) < 0.95, \n",
    "           1 - predictions_df[\"real_class\"])\n",
    "predictions_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[938,  44],\n",
       "       [  2,  16]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predictions_df[\"real_class\"], \n",
    "                 predictions_df[\"pred_e\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"a\", \"b\", \"c\", \"d\", \"e\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przegląd metryk klasyfikacji binarnej\n",
    "\n",
    "Przejdziemy po kolei przez powszechnie wykorzystywane metryki dla klasyfikacji binarnej.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Jest to najprostsza z metryk i bardzo intuicyjna, stąd też dość powszechna. Definiuje się ją jako stosunek prawidłowych predykcji, w stosunku do wszystkich predykcji.\n",
    "\n",
    "$$ accuracy = \\frac{TP + TN}{TP + FP + FN + TP} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model a: 0.483\n",
      "Model b: 0.967\n",
      "Model c: 0.982\n",
      "Model d: 0.018\n",
      "Model e: 0.954\n"
     ]
    }
   ],
   "source": [
    "for model in model_columns:\n",
    "    print(\n",
    "        f\"Model {model}:\", \n",
    "        accuracy_score(predictions_df[\"real_class\"], \n",
    "                       predictions_df[f\"pred_{model}\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okazuje się, że model, który jest zawsze negatywny, ma największą wydajność liczoną za pomocą metryki accuracy. Metryki tej używamy głównie dla zbalansowanych zbiorów oraz wtedy, gdy chcemy być w stanie wyjaśnić wszystkim jak dobry jest nasz model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision \n",
    "\n",
    "Miara ta wyznacza jak wiele obserwacji oznaczonych jako pozytywne, jest rzeczywiście pozytywne. W przypadku diagnozy naszej choroby, jest to stosunek liczby pacjentów poprawnie zdiagnozowanych jako chorzy, w stosunku do liczby wszystkich osób, dla których system przewidział klasę pozytywną.\n",
    "\n",
    "$$ precision = \\frac{TP}{TP + FP} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model a: 0.019267822736030827\n",
      "Model b: 0.058823529411764705\n",
      "Model c: 0.0\n",
      "Model d: 0.018\n",
      "Model e: 0.26666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukaw\\anaconda3\\envs\\intro-to-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in model_columns:\n",
    "    print(\n",
    "        f\"Model {model}:\", \n",
    "        precision_score(predictions_df[\"real_class\"], \n",
    "                        predictions_df[f\"pred_{model}\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optymalizujemy metrykę precision, jeśli zależy nam na wysokiej pewności, że osoby dla których wydaliśmy diagnozę *chory*, są rzeczywiście chorzy. Dopuszczamy jednak sytuację, że dla niektórych chorych nie będziemy w stanie poprawnie wykryć choroby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall / Sensitivity\n",
    "\n",
    "Jeśli chcielibyśmy obliczyć jak wiele elementów rzeczywiście pozytywnych zostało przez nas poprawnie zaklasyfikowane jako pozytywne, to recall jest odpowiednią metryką. W naszym przykładzie, zależałoby nam na wyłapaniu każdego chorego, ale nie zwracamy uwagi na to czy zdrowych pacjentów zaklasyfikujemy również jako chorych.\n",
    "\n",
    "$$ recall = \\frac{TP}{TP + FN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model a: 0.5555555555555556\n",
      "Model b: 0.05555555555555555\n",
      "Model c: 0.0\n",
      "Model d: 1.0\n",
      "Model e: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "for model in model_columns:\n",
    "    print(\n",
    "        f\"Model {model}:\", \n",
    "        recall_score(predictions_df[\"real_class\"],\n",
    "                     predictions_df[f\"pred_{model}\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optymalizacja tej metryki jest rozsądnym pomysłem, jeśli nie dbamy o koszta wykonania badań dla pacjentów którzy rzeczywiście są zdrowi, a zależy nam na wyłapaniu wszystkich chorych. To może być istotne, np. w przypadku chorób zakaźnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Score\n",
    "\n",
    "Metryki precision i recall mogą być naszym celem optymalizacji również łącznie. Jak porównać dwa modele na podstawie dwóch wartości? F-Score jest odpowiedzią, ponieważ łączy obie metryki.\n",
    "\n",
    "$$ fscore = (1 + \\beta^{2}) \\frac{precision \\cdot recall}{\\beta^{2} \\cdot precision + recall} $$\n",
    "\n",
    "Za pomocą parametru $ \\beta $ możemy sterować wagami każdej z metryk - im wyższa wartość $ \\beta $, tym bardziej dbamy o optymalizację metryki recall. Typowo stosuje się metrykę $ F1 $ oraz $ F2 $, gdzie parametr $ \\beta $ jest ustawiony na kolejno $ 1 $ i $ 2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model a: 0.037243947858472994\n",
      "Model b: 0.05714285714285714\n",
      "Model c: 0.0\n",
      "Model d: 0.03536345776031434\n",
      "Model e: 0.41025641025641024\n"
     ]
    }
   ],
   "source": [
    "for model in model_columns:\n",
    "    print(\n",
    "        f\"Model {model}:\", \n",
    "        fbeta_score(predictions_df[\"real_class\"],\n",
    "                    predictions_df[f\"pred_{model}\"], \n",
    "                    beta=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Szukanie optymalnego modelu\n",
    "\n",
    "Znajomość problemu może zasugerować nam na jakiego rodzaju błędy możemy się zgodzić. Machine Learning jest w stanie stworzyć modele, które będą najczęściej poprawne, jednak nie będą one perfekcyjne. Wybór odpowiedniej metryki pozwala wybrać model, który popełnia głównie nieszkodliwe dla nas pomyłki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model a: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.48      0.65       982\n",
      "           1       0.02      0.56      0.04        18\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.50      0.52      0.34      1000\n",
      "weighted avg       0.97      0.48      0.64      1000\n",
      "\n",
      "Model b: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       982\n",
      "           1       0.06      0.06      0.06        18\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.52      0.52      0.52      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "Model c: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       982\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.49      0.50      0.50      1000\n",
      "weighted avg       0.96      0.98      0.97      1000\n",
      "\n",
      "Model d: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       982\n",
      "           1       0.02      1.00      0.04        18\n",
      "\n",
      "    accuracy                           0.02      1000\n",
      "   macro avg       0.01      0.50      0.02      1000\n",
      "weighted avg       0.00      0.02      0.00      1000\n",
      "\n",
      "Model e: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       982\n",
      "           1       0.27      0.89      0.41        18\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.63      0.92      0.69      1000\n",
      "weighted avg       0.98      0.95      0.97      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukaw\\anaconda3\\envs\\intro-to-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in model_columns:\n",
    "    print(\n",
    "        f\"Model {model}:\", \"\\n\", \n",
    "        classification_report(predictions_df[\"real_class\"],\n",
    "                              predictions_df[f\"pred_{model}\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
